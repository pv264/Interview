## 1. Pods are Running but Application is Not Reachable via Service
# Kubernetes Troubleshooting

**Whatâ€™s really being tested:**
* Service selectors
* Endpoints
* Networking basics

First, Iâ€™ll check whether the Service has endpoints using:
`kubectl get endpoints <service-name>`

* If endpoints are empty, it usually means the **Service selector** does not match the **Pod labels**.
* Iâ€™ll verify labels on the pods using `kubectl get pods --show-labels` and compare them with the Service selector.
* If labels match and endpoints exist, Iâ€™ll then check the Service type, `targetPort` vs `containerPort`, and finally test connectivity using a temporary debug pod with `curl`.

---

**OR**

If a pod is running but the app isnâ€™t accessible:

1. I first verify the **container logs** and confirm the app is listening on the correct port.
2. Then I check the **Service `targetPort`** and ensure the application is bound to `0.0.0.0`, not `localhost`.

> In many cases, itâ€™s a port mismatch or binding issue.

## 2. Pod is Stuck in CrashLoopBackOff but Logs Look Fine
# CrashLoopBackOff Troubleshooting

**Whatâ€™s being tested:**
* Exit codes
* Liveness probes
* App startup behavior

**CrashLoopBackOff doesnâ€™t always mean the app is failing.**

1. **Check Exit Codes:**
   Iâ€™ll first check the container exit code using:
   `kubectl describe pod`

2. **Analyze Liveness Probes:**
   If the exit code is `0` or the logs look normal, I suspect a failing **liveness probe**.

3. **Verify Configuration:**
   Iâ€™ll verify the probe configuration:
   * `initialDelaySeconds`
   * `timeoutSeconds`
   * Whether the app is actually ready when the probe runs.

> **Note:** Often, aggressive liveness probes kill healthy containers during startup.

## 3. Kubernetes Node is NotReady Intermittently

**Whatâ€™s being tested:**
* Node health
* kubelet
* System resources

1. **Describe Node:**
   Iâ€™ll start by describing the node to see conditions using:
   `kubectl describe node`

2. **Common Causes:**
   Common causes are disk pressure, memory pressure, or kubelet issues.

3. **Node Access:**
   Then Iâ€™ll SSH into the node and check `kubelet` status, disk usage, and system logs.

4. **Disk Pressure:**
   If disk pressure is the issue, Iâ€™ll look for container logs or unused images consuming space.

> **Note:** Intermittent NotReady usually points to resource exhaustion or network instability.

---

## 4. Deployment Rollout is Stuck and Never Completes

**A rollout usually gets stuck because new pods are never becoming Ready.**

1. **Check Status:**
   Iâ€™ll run the following command and then inspect the new pods:
   `kubectl rollout status deployment`

2. **Inspect Issues:**
   Most of the time, the **readiness probe** is failing or the app is waiting for a dependency like a database.

> **Resolution:** Until readiness passes, Kubernetes wonâ€™t terminate old pods, so the rollout remains stuck.

---

## 5. Pod Cannot Pull Image but Node Has Internet Access

1. **Check Error:**
   First, Iâ€™ll check the exact error using:
   `kubectl describe pod`

2. **Authentication:**
   If itâ€™s an authentication error, Iâ€™ll verify the **ImagePullSecret** is created in the same namespace and referenced correctly in the pod spec.

3. **Cloud Registries (ECR/GCR):**
   If itâ€™s a cloud registry like ECR, Iâ€™ll ensure the node **IAM role** has permission to pull images.

> **Key Takeaway:** Internet access alone is not enough â€” registry auth is mandatory.

---

## 6. High Pod Restarts but No Errors in Logs

**When logs are clean, I immediately suspect OOMKilled.**

1. **Check State:**
   Iâ€™ll check pod events and container state for `OOMKilled`.

2. **The Cause:**
   If memory limits are too low, the kernel kills the process without app-level logs.

3. **The Fix:**
   The fix is either increasing memory limits or optimizing application memory usage.

---

## 7. Requests Are Slow Even Though Pods and Nodes Are Healthy

1. **Check CPU Limits:**
   Iâ€™ll first check CPU limits. Even if CPU usage looks low, strict CPU limits can cause **throttling**.

2. **Verify Metrics:**
   Using `kubectl top pod` and metrics, Iâ€™ll verify if pods are CPU throttled.

> **Optimization:** Often removing CPU limits or setting proper requests improves latency significantly.

---

## 8. Service Works Inside Cluster but Fails Externally

**If it works internally, the app and service are fine.**

1. **Focus Area:**
   Iâ€™ll then focus on the **Ingress** or external **Load Balancer**.

2. **Verification Points:**
   Iâ€™ll check listener ports, target group health checks, and security group rules.

> **Common Issue:** In cloud environments, missing inbound rules is a very common issue.

---

## 9. Pods Canâ€™t Communicate Across Namespaces

1. **Default Behavior:**
   By default, Kubernetes allows cross-namespace communication.

2. **Network Policies:**
   If itâ€™s failing, I immediately suspect **NetworkPolicies**.

3. **Troubleshooting:**
   Iâ€™ll check if thereâ€™s a deny-all policy applied and whether the required namespace selectors are missing.

> **Rule:** NetworkPolicies are namespace-scoped, so explicit allow rules are required once policies are enabled.

## 10. I have configured a desired replica count of 5 pods, but only 3 pods are running. What could be the issue?

**Core Issue:** If the desired replica count is 5 but only 3 are running, it usually means the remaining pods are in a **Pending** state and cannot be scheduled.

**Common Causes:**
1.  **Insufficient Resources:** Nodes do not have enough available **CPU or Memory** to meet the pod's requests.
2.  **Scheduling Constraints:** Pods are restricted by **Node Selectors** or **Affinity/Anti-Affinity** rules that cannot be satisfied.
3.  **Taints & Tolerations:** Nodes are tainted, and the new pods do not have the matching tolerations.
4.  **Resource Quotas:** The **Namespace** has hit its hard limit for CPU, memory, or pod count.
5.  **Pod Errors:**
    * **ImagePullBackOff:** Pods are scheduled but failing to download the image.
    * **CrashLoopBackOff:** Pods start but immediately crash.

**Troubleshooting Steps:**
* Check Pod Status: `kubectl get pods` (Look for Pending/Error).
* Check Events: `kubectl describe pod <pending-pod-name>` (The "Events" section will state exactly why it isn't scheduling).
* Check

## 11. what is OOM in kubernetes and why we get this error

# Kubernetes OOM (Out of Memory) Errors

In Kubernetes, **OOM** stands for **Out of Memory**. When you see this error (often appearing as `OOMKilled` in your pod status), it means a container was forcefully terminated because it tried to use more memory than it was allowed, or because the physical node it was running on ran out of RAM.

**Key Concept:**
Unlike CPU usage, which Kubernetes can "throttle" (slow down) to keep things running, **Memory is a non-compressible resource**. If a process needs memory and none is available, the system has no choice but to kill the process to protect the rest of the operating system.

## Why do you get this error?

There are two primary levels where an OOM event occurs:

### 1. Container Limit Exceeded (Cgroup OOM)
This is the most common cause. You have defined a limit in your YAML file, and the application tried to go past it.

* **The Cause:** Your application legitimately needed more memory for a task, or it has a **memory leak** where it gradually consumes RAM without releasing it.
* **The Result:** The Linux kernel's "OOM Killer" identifies the specific container breaching its limit and sends a `SIGKILL`.
* **How to spot it:**
    Running `kubectl describe pod <pod-name>` will show:
    * **Reason:** `OOMKilled`
    * **Exit Code:** `137`

    ## Scenario: 502 Errors After Kubernetes Deployment

**The Situation:**
* You deployed a new version of your application.
* Users are reporting **502 Bad Gateway** errors.
* Pods are in a `Running` state.
* The ALB (Application Load Balancer) is healthy.
* There are no obvious pod crashes.

**ðŸ‘‰ How to troubleshoot this step-by-step in production:**

**Strategy:** Isolate the issue layer by layer.

1.  **Confirm the Source (Ingress/ALB):**
    Check the ALB access logs to confirm exactly where the 502 is originating. Is the ALB generating it because the target closed the connection unexpectedly, or is the backend application explicitly returning a 502?
2.  **Verify Kubernetes Networking:**
    Check the **Service-to-Pod mapping**. Run `kubectl get endpoints <service-name>` to ensure the Service has actually registered the Pod IPs and is capable of routing traffic to them.
3.  **Validate Readiness Probes:**
    Ensure the readiness probes are configured correctly. If they are passing prematurely before the application has fully initialized, the Service will send traffic to a pod that isn't ready to accept connections, resulting in dropped requests.
4.  **Container-Level Testing:**
    `exec` into a pod (or use a temporary debug pod) to `curl` the application directly on its local port. This isolates whether the app itself is refusing connections or if the issue is strictly network-related.
5.  **Inspect Application Logs:**
    Run `kubectl logs <pod-name>` to look for backend failures. Look for things like database connection timeouts or application-level exceptions that don't crash the container process but cause it to fail to serve HTTP requests.
6.  **Compare Configurations:**
    Since this occurred immediately after a deployment, compare the YAML configurations (ConfigMaps, Secrets, environment variables) between the old and new versions to identify any breaking changes.
7.  **Mitigate (Rollback):**
    If the root cause isn't immediately obvious and production users are impacted, execute a rollback (`kubectl rollout undo deployment <name>`) to restore service quickly while investigating the faulty release in a lower environment.
